{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bart-text-summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38e2psgMZg8R",
        "outputId": "5379adb3-273c-435f-9e16-c97b499ef3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./ngrok: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-estimator-nightly==2.8.0.dev2021122109 earthengine-api==0.1.238 folium==0.2.1\n",
        "!pip install -q torchtext==0.11.0 torchaudio==0.10.0 torchvision==0.11.1 torch==1.10\n",
        "!pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/colab/1.10/torch_xla-1.10-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install -q transformers pytorch_lightning datasets pyngrok\n",
        "\n",
        "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
        "# !tar zxvf ngrok-stable-linux-amd64.tgz\n",
        "!./ngrok authtoken 1y2MQMr0xLh05Dvbb0dABiNQpAY_3bqEfwwEtM7duDaqwrN93"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bart.tokenization_bart_fast import BartTokenizerFast\n",
        "from datasets import load_dataset\n",
        "import torch.utils.data as data\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorboard import program\n",
        "from pyngrok import ngrok\n",
        "from pytorch_lightning import LightningModule, LightningDataModule, Trainer\n",
        "from transformers.models.bart.modeling_bart import BartForConditionalGeneration\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_CnBuLOZ8v8",
        "outputId": "0db4674a-afe4-45d0-eba3-4cd801a72d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
        "    \"\"\"\n",
        "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
        "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
        "\n",
        "    Args:\n",
        "        optimizer ([`~torch.optim.Optimizer`]):\n",
        "            The optimizer for which to schedule the learning rate.\n",
        "        num_warmup_steps (`int`):\n",
        "            The number of steps for the warmup phase.\n",
        "        num_training_steps (`int`):\n",
        "            The total number of training steps.\n",
        "        last_epoch (`int`, *optional*, defaults to -1):\n",
        "            The index of the last epoch when resuming training.\n",
        "\n",
        "    Return:\n",
        "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
        "    \"\"\"\n",
        "\n",
        "    def lr_lambda(current_step: int):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(\n",
        "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        )\n",
        "\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
      ],
      "metadata": {
        "id": "aMifJEVf6d3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_tensorboard(log_dir: str):\n",
        "    tb = program.TensorBoard()\n",
        "    tb.configure(argv=[None, '--logdir', log_dir])\n",
        "    url = tb.launch()\n",
        "    print(f\"Tensorflow listening on {url}\")\n",
        "    port = int(url.split(\":\")[-1][:-1])\n",
        "    print(ngrok.connect(port))"
      ],
      "metadata": {
        "id": "69gsrXAT6L4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(data.Dataset):\n",
        "    tokenizer = BartTokenizerFast.from_pretrained(\"sshleifer/distilbart-xsum-12-1\")\n",
        "\n",
        "    def __init__(self, split: str = \"train\"):\n",
        "     \n",
        "        self.data = pd.read_csv(f\"Data/{split}.csv\").drop(\"id\", axis=1)\n",
        "        self.data = self.data.to_numpy()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        document, summary = self.data[idx]\n",
        "        input_ids, attention_mask =  tuple(self.tokenizer(document, padding=\"max_length\", truncation=True, return_tensors=\"pt\").values())\n",
        "        decoder_input_ids, decoder_attention_mask =  tuple(self.tokenizer(summary, padding=\"max_length\", truncation=True, return_tensors=\"pt\").values())\n",
        "        return (input_ids.squeeze(0), attention_mask.squeeze(0)), (decoder_input_ids.squeeze(0), decoder_attention_mask.squeeze(0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "3MEin9_l43lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataModule(LightningDataModule):\n",
        "    def __init__(self, batch_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.train_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.val_dataset = None\n",
        "\n",
        "    def prepare_data(self) -> None:\n",
        "        if not os.path.exists(\"Data\"):\n",
        "            os.mkdir(\"Data\")\n",
        "            datasets = load_dataset(\"xsum\", name=\"bart-base\")\n",
        "\n",
        "            for split in (\"train\", \"test\", \"validation\"):\n",
        "                datasets[split].to_csv(f\"Data/{split}.csv\", index=False)\n",
        "                pd.read_csv(f\"Data/{split}.csv\").dropna().to_csv(f\"Data/{split}.csv\", index=False)\n",
        "\n",
        "\n",
        "    def setup(self, stage: str = None) -> None:\n",
        "        if (stage == \"fit\" or stage == None) and ((not self.train_dataset) or (not self.val_dataset)):\n",
        "            self.train_dataset = Dataset(split=\"train\")\n",
        "            self.val_dataset = Dataset(split=\"validation\")\n",
        "       \n",
        "        if (stage == \"test\" or stage == None) and (not self.test_dataset):\n",
        "            self.test_dataset = Dataset(split=\"test\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return data.DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return data.DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return data.DataLoader(self.test_dataset, batch_size=self.batch_size)"
      ],
      "metadata": {
        "id": "vEeQD3dx496g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(LightningModule):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-xsum-12-1\")\n",
        "\n",
        "    @staticmethod\n",
        "    def shift_right(tensor: torch.Tensor, start_token: int):\n",
        "        shifted = torch.zeros_like(tensor)\n",
        "\n",
        "        shifted[:, 1:] = tensor[:, :-1].clone()\n",
        "        shifted[:, 0] = start_token\n",
        "        return shifted\n",
        "        \n",
        "    def forward(self, batch: tuple):\n",
        "        (input_ids, attention_mask), (decoder_ids, decoder_attention_mask) = batch\n",
        "        # decoder_ids.shape: (batch_size, seq_len)\n",
        "        decoder_inps = self.shift_right(decoder_ids, Dataset.tokenizer.bos_token_id)\n",
        "        \n",
        "        decoder_attention_mask = self.shift_right(decoder_attention_mask, 1)\n",
        "        logits: torch.Tensor = self.model(input_ids, attention_mask, decoder_inps, decoder_attention_mask).logits\n",
        "        # logits.shape: (batch_size, seq_len, vocab_size)\n",
        "        # logits obviously means no activation\n",
        "        return logits"
      ],
      "metadata": {
        "id": "BmJMzHSk52aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainModel(Model):\n",
        "    def __init__(self, learning_rate: float, ultimate_batch_size: int, epochs: int, label_smoothing=0.0, ignore_index=None):\n",
        "        super(TrainModel, self).__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing, ignore_index=ignore_index)\n",
        "\n",
        "        steps_per_iter = 215344 # len(train_dataset) + len(val_dataset)\n",
        "        self.num_training_steps = (steps_per_iter // ultimate_batch_size) * epochs\n",
        "        self.num_warmup_steps = int(self.num_training_steps * 0.1)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, self.num_warmup_steps, self.num_training_steps)\n",
        "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def calculate_accuracy(logits: torch.Tensor, target: torch.Tensor):\n",
        "        # logits.shape: (batch_size, seq_len, vocab_size)\n",
        "        # decoder_ids.shape: (batch_size, seq_len)\n",
        "        predictions: torch.Tensor = F.softmax(logits, dim=2).argmax(axis=2)\n",
        "        # predictions.shape: (batch_size, seq_len)\n",
        "        accuracy = (target.view(-1) == predictions.view(-1)).sum() / (predictions.size(0) * predictions.size(1))\n",
        "        return accuracy\n",
        "\n",
        "    def forward_step(self, batch: tuple):\n",
        "        _, (decoder_ids, _) = batch\n",
        "        # decoder_ids.shape: (batch_size, seq_len)\n",
        "\n",
        "        logits = self.forward(batch)\n",
        "        # logits.shape: (batch_size, seq_len, vocab_size)\n",
        "        loss: torch.Tensor = self.criterion(logits.reshape(logits.size(0) * logits.size(1), logits.size(2)), decoder_ids.reshape(decoder_ids.size(0) * decoder_ids.size(1)))\n",
        "        accu = self.calculate_accuracy(logits, decoder_ids)\n",
        "        return loss, accu.item()\n",
        "\n",
        "    def training_step(self, batch: tuple, batch_idx: int):\n",
        "        loss, accu = self.forward_step(batch)\n",
        "        self.log(\"lr\", self.lr_schedulers().get_last_lr()[0], prog_bar=True)\n",
        "        self.log(\"loss\", loss.item())\n",
        "        self.log(\"accu\", accu, prog_bar=True)\n",
        "        # print(f\"train step done Loss={loss.item()} !\")\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch: tuple, batch_idx: int):\n",
        "        loss, accu = self.forward_step(batch)\n",
        "        self.log(\"val_loss\", loss.item(), prog_bar=True)\n",
        "        self.log(\"val_accu\", accu, prog_bar=True)\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch: tuple, batch_idx: int):\n",
        "        loss, accu = self.forward_step(batch)\n",
        "        self.log(\"test_loss\", loss.item(), prog_bar=True)\n",
        "        self.log(\"test_accu\", accu, prog_bar=True)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "hiMfhLon6zm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "batch_size = 1\n",
        "learning_rate = 1e-6\n",
        "tpu_cores = 8\n",
        "\n",
        "\n",
        "trainer = Trainer(max_epochs=epochs, tpu_cores=tpu_cores)\n",
        "model = TrainModel(learning_rate, batch_size*tpu_cores, epochs, label_smoothing=0.1, ignore_index=Dataset.tokenizer.pad_token_id)\n",
        "datamodule = DataModule(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDpoUISa60uN",
        "outputId": "84b0f931-3397-4e30-af2b-295747fd9c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: True, using: 8 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.fit(model, datamodule)"
      ],
      "metadata": {
        "id": "LevIrQva_fve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datamodule.setup()\n",
        "batch = next(iter(datamodule.train_dataloader()))"
      ],
      "metadata": {
        "id": "jU4mz-elor1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(batch)"
      ],
      "metadata": {
        "id": "ggdnIfwq0d1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlFtGWfJ0e61",
        "outputId": "4fbf8a6a-f155-4aa9-9235-01fadfe1654d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024, 50264])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIpY8yKo1zE_",
        "outputId": "947ad283-6bfa-4b67-e492-3df9c1a4987f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0, 133, 455,  ...,   1,   1,   1]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_vocab = {i: k for k, i in Dataset.tokenizer.vocab.items()}"
      ],
      "metadata": {
        "id": "NGZYux4v13Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str('ĠworstĠ').strip(\"Ġ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BDpWkAOf3dCF",
        "outputId": "c5c1e969-94f5-460a-b51c-5b58dff5b678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'worst'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join([my_vocab[i].strip(\"Ġ\").strip(\"Ċ\") for i in batch[0][0].squeeze(0).tolist()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "5Db_9ZMj3Apl",
        "outputId": "f232fb52-4925-461f-e76d-5a65e6329772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s> The full cost of damage in Newton Stewart , one of the areas worst affected , is still being assessed .  Rep air work is ongoing in Haw ick and many roads in P ee b less hire remain badly affected by standing water .  Tr ains on the west coast mainline face disruption due to damage at the L aming ton Vi ad uct .  Many businesses and household ers were affected by flooding in Newton Stewart after the River Cree overfl owed into the town .  First Minister Nicola Sturgeon visited the area to inspect the damage .  The waters breached a retaining wall , flooding many commercial properties on Victoria Street - the main shopping thorough fare .  Jean ette Tate , who owns the Cinnamon Cafe which was badly affected , said she could not fault the multi - agency response once the flood hit .  However , she said more prevent ative work could have been carried out to ensure the retaining wall did not fail .  \" It is difficult but I do think there is so much publicity for Dum f ries and the N ith - and I totally appreciate that - but it is almost like we \\'re neglected or forgotten ,\" she said .  \" That may not be true but it is perhaps my perspective over the last few days .  \" Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out ?\"  Meanwhile , a flood alert remains in place across the Borders because of the constant rain .  P ee bles was badly hit by problems , sparking calls to introduce more defences in the area .  Scott ish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs .  The Labour Party \\'s deputy Scottish leader Alex Row ley was in Haw ick on Monday to see the situation first hand .  He said it was important to get the flood protection plan right but backed calls to speed up the process .  \" I was quite taken aback by the amount of damage that has been done ,\" he said .  \" Obviously it is heart - breaking for people who have been forced out of their homes and the impact on businesses .\"  He said it was important that \" im mediate steps \" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans .  Have you been affected by flooding in Dum f ries and Gall oway or the Borders ? Tell us about your experience of the situation and how it was handled . Email us on se l k irk . news @ bb c . co . uk or d um f ries @ bb c . co . uk . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch[1][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC_S0lcy3OOM",
        "outputId": "a1b7d1cf-1451-41d9-ef83-01d7daaebf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sum = out.argmax(-1).squeeze(0)\n",
        "true_sum = batch[1][0].squeeze(0)"
      ],
      "metadata": {
        "id": "FVpjHuY-32Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(pred_sum[true_sum != 1] == true_sum[true_sum != 1]).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1H7jASN4DDi",
        "outputId": "fe4f0d43-7d32-4720-eafa-abafa3fa462b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(12)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sum[true_sum != 1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz9pY-fG4EQ4",
        "outputId": "7899976c-35e9-4538-da75-4996783bfc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([26])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uk4H8TFD4hN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-oM2I6g4T7Z",
        "outputId": "837d52b2-dd09-4249-d5ca-034f16d82a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    0, 40827,    12,  ...,     1,     1,     1])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IDKC3Qe94Ywd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}